{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_sentiment_prediction_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perezsam/DM19-Lab2-Homework/blob/master/twitter_sentiment_prediction_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vaq11njAWtZ",
        "colab_type": "code",
        "outputId": "4162f7e8-fb7a-487d-ed53-2985b26298cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=52cea8f65d5c3d9a6cf8a782bf878bbb64a2887ab34041d647a43fd9a49ac493\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYK4uzCjmJWr",
        "colab_type": "code",
        "outputId": "b63f9d12-2f2b-4ee2-c301-12c7d0c14de4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import json\n",
        "import keras\n",
        "import keras.preprocessing.text as kpt\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "from time import time\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import emoji\n",
        "from pprint import pprint\n",
        "import collections\n",
        "\n",
        "sns.set(style=\"darkgrid\")\n",
        "sns.set(font_scale=1.3)\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "import gensim\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "np.random.seed(37)\n",
        "KAGGLE_ENV = os.getcwd == '/kaggle/working'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11r5QpoRnSQr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from pandas.io.json import json_normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wFVeRBFGujp",
        "colab_type": "text"
      },
      "source": [
        "##Loading raw dataset from kaggle##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar00Ioy1mJoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNGcVm2Ai54q",
        "colab_type": "code",
        "outputId": "3793d03e-874a-47bb-a3c5-6fce5f8b8e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 64 Nov 27 21:07 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMW3QEF4i6I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-6yMDu3i6GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ketIeFyhi6Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List available datasets.\n",
        "#!kaggle datasets list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttupdoy1i6BC",
        "colab_type": "code",
        "outputId": "e1b7a3ca-92b2-4c1b-fd92-7911ba95a8c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Copy the private data set locally.\n",
        "!kaggle datasets download -d cssamuel/dmlab2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dmlab2.zip to /content\n",
            " 90% 134M/149M [00:00<00:00, 178MB/s]\n",
            "100% 149M/149M [00:00<00:00, 192MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-jG-kVti5-j",
        "colab_type": "code",
        "outputId": "38cea2b4-c9fd-4041-9f69-9bc176d01e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  dmlab2.zip  kaggle.json  \u001b[01;34moutput\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aST-lwJ0jhse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv dmlab2.zip ./data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZtk7w8CjKee",
        "colab_type": "code",
        "outputId": "b1f37d97-09ca-4505-9d72-21a1e5a11352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2JcXqaUjKoe",
        "colab_type": "code",
        "outputId": "a8a7a9e2-1eba-4aa8-fadb-1726383bd0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMMSD1xFjKsr",
        "colab_type": "code",
        "outputId": "eaa6a676-2e42-4093-cd2a-ea057139bb1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!unzip dmlab2.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  dmlab2.zip\n",
            "  inflating: data_identification.csv  \n",
            "  inflating: emotion.csv             \n",
            "  inflating: sampleSubmission.csv    \n",
            "  inflating: tweets_DM.json          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgQVNZOkjKky",
        "colab_type": "code",
        "outputId": "9d3e9aa3-fcdd-414a-aefd-f2b4cfcc6eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z36EY0XKjKip",
        "colab_type": "code",
        "outputId": "9641afb9-0821-424d-e521-a77068fa021f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1c-xKv-i58w",
        "colab_type": "code",
        "outputId": "779cf0e9-7757-431d-cd8b-f67e5c323f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  kaggle.json  \u001b[01;34moutput\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFSIVUxjG4F2",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzvApq0YkUxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read data files\n",
        "raw_data=pd.read_json(\"./data/tweets_DM.json\",lines=True)\n",
        "tweets=json_normalize(data=raw_data['_source'])\n",
        "identify=pd.read_csv(\"./data/data_identification.csv\")\n",
        "emotion=pd.read_csv(\"./data/emotion.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpriMpmtkU_q",
        "colab_type": "code",
        "outputId": "f853e127-a7af-4247-aaef-0c70945d5485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h10Wv2szkVDv",
        "colab_type": "code",
        "outputId": "5c48147e-0978-4401-8560-b462a18beafb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# rename column names\n",
        "tweets=tweets.rename(index=str,columns={\"tweet.text\":\"text\", \"tweet.tweet_id\":\"tweet_id\",\n",
        "                                       \"tweet.hashtags\":\"hashtags\"})\n",
        "# add identify tags to dataframe\n",
        "tweets=pd.merge(tweets,identify, on=\"tweet_id\")\n",
        "\n",
        "#get training set and test set\n",
        "train_df=tweets[tweets[\"identification\"] == \"train\"]\n",
        "test_df=tweets[tweets[\"identification\"] == \"test\"]\n",
        "\n",
        "#add emotion column\n",
        "train_df=pd.merge(train_df,emotion, on=\"tweet_id\")\n",
        "#test_df[\"emotion\"]=\"\"\n",
        "\n",
        "#drop identification tags\n",
        "train_df.drop(columns=[\"identification\"],inplace=True)\n",
        "test_df.drop(columns=[\"identification\"],inplace=True)\n",
        "\n",
        "#use tweet_id as index\n",
        "train_df.set_index(\"tweet_id\",inplace=True)\n",
        "test_df.set_index(\"tweet_id\",inplace=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4117: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpdKb60oHA_C",
        "colab_type": "text"
      },
      "source": [
        "Save training and testing dataset to a pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqJWvJKnkVL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save to pickle file\n",
        "train_df.to_pickle(\"./output/train_df.pkl\")\n",
        "test_df.to_pickle(\"./output/test_df.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcYZKnYaHJwQ",
        "colab_type": "text"
      },
      "source": [
        "Load data training data for our Neural Network model, selecting only useful data (i.e. text and emotion)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMLq07Z2X_0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## load a pickle file\n",
        "df = pd.read_pickle(\"./output/train_df.pkl\")\n",
        "df = df.reindex(np.random.permutation(df.index))  \n",
        "df = df[['text', 'emotion']]\n",
        "\n",
        "## select a sample of 250,000 due time and memory constraint in google colab\n",
        "df = df.sample(250000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBxTQfGaWkM0",
        "colab_type": "code",
        "outputId": "fb2160d0-a186-421a-da92-20c780ab8bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## same class used in simple text minig to clean training and testing data set\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class CleanText(BaseEstimator, TransformerMixin):\n",
        "   \n",
        "    def remove_mentions(self, input_text):\n",
        "        return re.sub(r'@\\w+', '', input_text)\n",
        "    \n",
        "    def remove_urls(self, input_text):\n",
        "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
        "    \n",
        "    def emoji_oneword(self, input_text):\n",
        "        # By compressing the underscore, the emoji is kept as one word\n",
        "        return input_text.replace('_','')\n",
        "    \n",
        "    def remove_punctuation(self, input_text):\n",
        "        # Make translation table\n",
        "        punct = string.punctuation\n",
        "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
        "        return input_text.translate(trantab)\n",
        "\n",
        "    def remove_digits(self, input_text):\n",
        "        return re.sub('\\d+', '', input_text)\n",
        "    \n",
        "    def to_lower(self, input_text):\n",
        "        return input_text.lower()\n",
        "    \n",
        "    def remove_stopwords(self, input_text):\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words) \n",
        "    \n",
        "    def stemming(self, input_text):\n",
        "        porter = PorterStemmer()\n",
        "        words = input_text.split() \n",
        "        stemmed_words = [porter.stem(word) for word in words]\n",
        "        return \" \".join(stemmed_words)\n",
        "    \n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, **transform_params):\n",
        "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
        "        return clean_X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGwNcBMUI1r5",
        "colab_type": "text"
      },
      "source": [
        "### Text cleaning for training dataset ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIXGPjP5M6CE",
        "colab_type": "code",
        "outputId": "813c15f5-c04c-4333-f878-ad7963d8a410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "ct = CleanText()\n",
        "sr_clean = ct.fit_transform(df.text)\n",
        "sr_clean.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweet_id\n",
              "0x341116    hmm mayb lh                                                                \n",
              "0x227e4b    like eat organ clean make bodi feel free less bodi tension build also go lh\n",
              "0x21483d    miss stream work kill lh                                                   \n",
              "0x36a7f8    look forward explor new look later today lh plush                          \n",
              "0x21b2a5    believ lh lh                                                               \n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkgJ10D4NNEK",
        "colab_type": "code",
        "outputId": "6ee93d5b-af56-474f-80b5-5d9e813ec6b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "empty_clean = sr_clean == ''\n",
        "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
        "sr_clean.loc[empty_clean] = '[no_text]'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 records have no words left after text cleaning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffWBCkEWVsux",
        "colab_type": "code",
        "outputId": "2e44d989-d379-4415-fff6-ffc333d9ae2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_model = df\n",
        "df_model['clean_text'] = sr_clean\n",
        "df_model.columns.tolist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text', 'emotion', 'clean_text']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W_hxsE6Vs86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_text_train = df_model[['clean_text', 'emotion']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H_Esdp9VPYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_text_train.to_csv('./data/train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N-RtMwFODYE",
        "colab_type": "code",
        "outputId": "2f996d39-72ec-4dd6-e433-19667836b3f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "pd.read_csv('./data/train.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0x2cad92</td>\n",
              "      <td>shoot vega sad pray lh la vega lh shoot vega violenc</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0x372564</td>\n",
              "      <td>uni skip class studi midterm lh</td>\n",
              "      <td>anticipation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0x1f3a0a</td>\n",
              "      <td>close sell lot eurusd pip total today pip lh lh lh lh lh</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0x2508e3</td>\n",
              "      <td>bore realli feel like move bore lh ohioproblem</td>\n",
              "      <td>disgust</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0x242436</td>\n",
              "      <td>miss much lh wanna one 흥얼흥얼</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249995</th>\n",
              "      <td>0x26525d</td>\n",
              "      <td>slot left ☺️ dm order twice twicetagram aroha astro lh init carpediem epikhigh wevedonesomethingwond</td>\n",
              "      <td>anticipation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249996</th>\n",
              "      <td>0x2850b6</td>\n",
              "      <td>never let ladi sleep lh sad</td>\n",
              "      <td>sadness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249997</th>\n",
              "      <td>0x32f577</td>\n",
              "      <td>launch day today dplaunch lh winerecept prioriti</td>\n",
              "      <td>anticipation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249998</th>\n",
              "      <td>0x293c3b</td>\n",
              "      <td>must one mani brat lh titan</td>\n",
              "      <td>surprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249999</th>\n",
              "      <td>0x2ca4b2</td>\n",
              "      <td>cat shave lh movemb</td>\n",
              "      <td>joy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        tweet_id  ...       emotion\n",
              "0       0x2cad92  ...  sadness     \n",
              "1       0x372564  ...  anticipation\n",
              "2       0x1f3a0a  ...  joy         \n",
              "3       0x2508e3  ...  disgust     \n",
              "4       0x242436  ...  joy         \n",
              "...          ...  ...  ...         \n",
              "249995  0x26525d  ...  anticipation\n",
              "249996  0x2850b6  ...  sadness     \n",
              "249997  0x32f577  ...  anticipation\n",
              "249998  0x293c3b  ...  surprise    \n",
              "249999  0x2ca4b2  ...  joy         \n",
              "\n",
              "[250000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKRDg62LdR2z",
        "colab_type": "text"
      },
      "source": [
        "##Text cleaning for testing dataset##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSMa1Dn0baLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## load a pickle file\n",
        "model_test_df = pd.read_pickle(\"./output/test_df.pkl\")\n",
        "#df = df.reindex(np.random.permutation(df.index))  \n",
        "model_test_df = model_test_df[['text', 'emotion']]\n",
        "#model_test_df = model_test_df.sample(50000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWY7Xcctbaw5",
        "colab_type": "code",
        "outputId": "01747350-4162-412c-ad72-94f1a1a403c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "sr_clean_test = ct.fit_transform(model_test_df.text)\n",
        "sr_clean_test.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tweet_id\n",
              "0x2c18a7    alreadi saw least fan anoth group claim comeback trailer better comeback mv 😄😊 lh \n",
              "0x355c58    hommi daxton download game onto xbox via wifi thank use fast wifi month lh        \n",
              "0x332513    know ur peopl stop react gonegirl overit lh                                       \n",
              "0x20db8b    trump crazi seri tweet puerto rico dedic golf trophi peopl realli itmfa surreal lh\n",
              "0x31abec    god keep bless first new place start work tomorrow 🙏🏽❣️ lh lh                     \n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G42gsnIba8j",
        "colab_type": "code",
        "outputId": "6a85baf0-afbb-408c-cf13-5623759b3a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "empty_clean_t = sr_clean_test == ''\n",
        "print('{} records have no words left after text cleaning'.format(sr_clean_test[empty_clean_t].count()))\n",
        "sr_clean_test.loc[empty_clean_t] = '[no_text]'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 records have no words left after text cleaning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFFHzmDUbbFt",
        "colab_type": "code",
        "outputId": "e8aa343f-db1a-4005-f9ee-fe92879e7d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_model_test = model_test_df\n",
        "df_model_test['clean_text'] = sr_clean_test\n",
        "df_model_test.columns.tolist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text', 'emotion', 'clean_text']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17hIW1kMbbPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_text_test = df_model_test[['clean_text']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZHLMKc8d1xC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_text_test.to_csv('./data/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKxorAb1eapH",
        "colab_type": "text"
      },
      "source": [
        "##Train Neural Network##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4qdTMJ5AfrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_text_to_index_array(text):\n",
        "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\n",
        "\n",
        "def data_prepare(training_file_path):\n",
        "\n",
        "    dataset = pd.read_csv(training_file_path)\n",
        "    reviews = []\n",
        "    labels = []    \n",
        "    \n",
        "    # Enconding Categorical Data     \n",
        "    labelencoder_y = LabelEncoder()\n",
        "    dataset['emotion'] = labelencoder_y.fit_transform(dataset['emotion'])\n",
        "    cLen = len(dataset['clean_text'])\n",
        "        \n",
        "    for i in range(0,cLen):\n",
        "        review = dataset['clean_text'][i]\n",
        "        reviews.append(review) \n",
        "        label = dataset[\"emotion\"][i]\n",
        "        labels.append(label)    \n",
        "    labels = np.asarray(labels)\n",
        "    return reviews,labels\n",
        "\n",
        "\n",
        "train_file_path = \"./data/train.csv\"\n",
        "[reviews,labels] = data_prepare(train_file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlCgJWgmAjNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Dictionary of words and their indices\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "dictionary = tokenizer.word_index\n",
        "\n",
        "# save dictionary\n",
        "with open('dictionary.json','w') as dictionary_file:\n",
        "    json.dump(dictionary,dictionary_file)\n",
        "\n",
        "# Replace words of each text review to indices\n",
        "allWordIndices = []\n",
        "for num,text in enumerate(reviews):\n",
        "    wordIndices = convert_text_to_index_array(text)\n",
        "    allWordIndices.append(wordIndices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py9KHsrp-Uhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#OUT OF MEMORY\n",
        "# Convert the index sequences into binary bag of words vector (one hot encoding) \n",
        "allWordIndices = np.asarray(allWordIndices)\n",
        "train_X = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\n",
        "labels = keras.utils.to_categorical(labels,num_classes=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeKCZWKL7mi7",
        "colab_type": "code",
        "outputId": "7efd68e2-4486-448e-a181-7805fdcadd84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "# Creating Dense Neural Networl Model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(max_words,), activation='elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation='elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "\n",
        "print (model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "  optimizer='sgd',\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 256)               2560256   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 2,594,184\n",
            "Trainable params: 2,594,184\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tn1R1ms7mag",
        "colab_type": "code",
        "outputId": "46fa086b-1bb6-488b-8338-821c990ac09f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "# Training the Model\n",
        "model.fit(train_X, labels,\n",
        "  batch_size=32,\n",
        "  epochs=10,\n",
        "  verbose=1,\n",
        "  validation_split=0.1,\n",
        "  shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 225000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "225000/225000 [==============================] - 42s 187us/step - loss: 1.6556 - acc: 0.4005 - val_loss: 1.5337 - val_acc: 0.4364\n",
            "Epoch 2/10\n",
            "225000/225000 [==============================] - 37s 163us/step - loss: 1.5060 - acc: 0.4540 - val_loss: 1.4568 - val_acc: 0.4705\n",
            "Epoch 3/10\n",
            "225000/225000 [==============================] - 37s 162us/step - loss: 1.4497 - acc: 0.4763 - val_loss: 1.4201 - val_acc: 0.4840\n",
            "Epoch 4/10\n",
            "225000/225000 [==============================] - 36s 162us/step - loss: 1.4167 - acc: 0.4882 - val_loss: 1.4045 - val_acc: 0.4889\n",
            "Epoch 5/10\n",
            "225000/225000 [==============================] - 36s 162us/step - loss: 1.3956 - acc: 0.4965 - val_loss: 1.3928 - val_acc: 0.4983\n",
            "Epoch 6/10\n",
            "225000/225000 [==============================] - 36s 162us/step - loss: 1.3795 - acc: 0.5035 - val_loss: 1.3818 - val_acc: 0.5022\n",
            "Epoch 7/10\n",
            "225000/225000 [==============================] - 36s 162us/step - loss: 1.3666 - acc: 0.5084 - val_loss: 1.3776 - val_acc: 0.5057\n",
            "Epoch 8/10\n",
            "225000/225000 [==============================] - 37s 164us/step - loss: 1.3555 - acc: 0.5127 - val_loss: 1.3845 - val_acc: 0.5040\n",
            "Epoch 9/10\n",
            "225000/225000 [==============================] - 36s 161us/step - loss: 1.3467 - acc: 0.5148 - val_loss: 1.3720 - val_acc: 0.5076\n",
            "Epoch 10/10\n",
            "225000/225000 [==============================] - 36s 161us/step - loss: 1.3380 - acc: 0.5172 - val_loss: 1.3728 - val_acc: 0.5063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e8f36dc88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aflk9SEt7mSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model to disk\n",
        "model_json = model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.save_weights('model.h5')   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6Tof5-CkkM",
        "colab_type": "text"
      },
      "source": [
        "##Predict on testing data##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm3eG8Im7l5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import model_from_json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJvYiyWx-NFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_text_to_index_array(text):\n",
        "    words = kpt.text_to_word_sequence(text)\n",
        "    wordIndices = []\n",
        "    for word in words:\n",
        "        if word in dictionary:\n",
        "            wordIndices.append(dictionary[word])\n",
        "    return wordIndices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv5_vpmX-NBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the dictionary\n",
        "labels = [\"anger\", \"anticipation\", \"disgust\",\t\"fear\",\t\"joy\", \"sadness\", \"surprise\", \"trust\"]\n",
        "with open('dictionary.json', 'r') as dictionary_file:\n",
        "    dictionary = json.load(dictionary_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6s4yTj6-M9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load trained model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)\n",
        "model.load_weights('model.h5')\n",
        "\n",
        "testset = pd.read_csv(\"./data/test.csv\")    \n",
        "cLen = len(testset['clean_text'])\n",
        "tokenizer = Tokenizer(num_words=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWAMxLA8u4zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict happiness for each review in test.csv\n",
        "y_pred = []   \n",
        "for i in range(0,cLen):\n",
        "    review = testset['clean_text'][i]\n",
        "    testArr = convert_text_to_index_array(review)   \n",
        "    input = tokenizer.sequences_to_matrix([testArr], mode='binary')\n",
        "    pred = model.predict(input)\n",
        "    #print pred[0][np.argmax(pred)] * 100, labels[np.argmax(pred)]\n",
        "    y_pred.append(labels[np.argmax(pred)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoHd5AYHu4-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the results in submission csv file\n",
        "raw_data = {'id': testset['tweet_id'], \n",
        "        'emotion': y_pred}\n",
        "df = pd.DataFrame(raw_data, columns = ['id', 'emotion'])\n",
        "df.to_csv('submission_model1.csv', sep=',',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}