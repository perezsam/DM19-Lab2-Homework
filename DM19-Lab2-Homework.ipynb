{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Samuel Perez\n",
    "\n",
    "Student ID: 107065434\n",
    "\n",
    "GitHub ID: perezsam\n",
    "\n",
    "Kaggle name: transcendence\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![Snapshot](img/pic0.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/submissions.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM19-Lab2-Master Repo](https://github.com/EvaArevalo/DM19-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/179d01d4dd984fc5ac45a894822479dd) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 23rd 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/EvaArevalo/DM19-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM19-Lab2-Homework](https://github.com/EvaArevalo/DM19-Lab2-Homework) repository this time! Also please __DON´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different notebooks were created to keep the different tasks organized**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Completed **take home** exercises can be found in the following notebook [Lab 2 - Take home exercises](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/lab2_take_home_exercises.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "**Kaggle Competition:** Refer to the the leaderboard position showed in the student information section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "I tried different models when working on the competition, to avoid having a big notebook, a link to each notebook will be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using simple text mining techniques: ###\n",
    "The following notebook [Using text mining techniques](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/kaggle_competition_simple_text_mining.ipynb) include everything from raw data preprocessing, data exploration, text cleaning and grid search for finding the best tuning parameters for the two selected models (i.e MultinomailNB and LogisticRegression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using GridSearchCV from sklearn we found that CountVectorizer was the best choice compared with TF-IDF when working with the LogisticRegression and MultinomailNB models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we found the best tunning parameters we trained our model and got 45.3% and 46.11% in the leaderboard for MultinomailNB and LogisticRegression respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-layered fully connected Neural Network ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried building a Multi-layered fully connected Neural Network, however I was relying only on google colab for building and training the models since I do not have access to a computer with good computational resources.\n",
    "\n",
    "I got the problem of running out of memory, so I couldn't train my model with the full training dataset, besides that the training can take a long time and I could easily loss my work if I get disconnected from the colab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following figure we can see that the public score (second column) remains at 40% and there is no significant improvement when increasing the training set sample by a few thousands, except for the improvement from 38% to 40% using a training sample of 50,000 and 90,000 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/nn_model.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a training sample of 250,000 I couldn’t go further due to memory and time constraints from google colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook of the Neural Network model can be found here, [NN twitter sentiment prediction](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/twitter_sentiment_prediction_NN.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LogisticRegression, TweetTokenizer and TfidfVectorizer ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some research I found out that nltk has a TwitterTokenizer wich is a specialized tokenizer for tweets analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to consider the following important points for Feature Engineering in our selected model.\n",
    "\n",
    "**Vectorize**\n",
    "After tokenizing, we have to use a vectorizer to get angument term matrices (usually sparse). Then angumnt matrices can be sent to a machince learning or deep learning model as inputs. There are two steps in vectorizing processes: fitting and transforming . In fitting part, vectorizers will extract every features from text we provided. In trasformer part, transformer will count frequncies of every features showing up in text, and give a spasrse matrix.\n",
    "\n",
    "**CountVectorizer**\n",
    "BOW model only consider the frequncies of every fetures in texts. The higher the frequencies, the higher the weights. CountVectorizer can be used to implement both BOW (bag of words) and TFIDF (term frequencies inverse document frequencies, with a TFIDF transformer) models. After fitting, we can apply a CountTransformer to get BOW matrices of traning and testing sets.\n",
    "\n",
    "**TfidfVectorizer = CountVectorizer + TfidfTransformer**\n",
    "Since BOW model only consider the frequncies of every fetures in texts, it may lose some context and meaning infromation. TFIDF model not only consider term frequencies in current texts, it also take term frequencies in other texts. If a word is common in every text, for example, \"Friday\", it may be not so important for distinguish emotions. TFIDF will reduce the weights of those common features to make other special important term higher weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected model**\n",
    "\n",
    "**Logistic Regression** is a type of regression that predicts the probability of ocurrence of an event by fitting data to a logit function (logistic function). Like many forms of regression analysis, it makes use of several predictor variables that may be either numerical or categorical. For instance, the probability that a person has a heart attack within a specified time period might be predicted from knowledege of the person's age, sex and body mass index. This regression is quite used in several scenarios such as prediction of customer's propensity to purchase a product or cease a subscription in marketing applications and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: [Machine Learning with Python - Logistic Regression](http://aimotion.blogspot.com/2011/11/machine-learning-with-python-logistic.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some experiments with the lab dataset to check the performance the LogisticRegression model using TwitterTokenizer and TfidfVectorizer we selected the most appropriate parameters to train the model for emotion prediction in the kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a score of 46% in the Kaggle competition, and there was no significant improvement when using a higher max_features for the TfidfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/twitter_tokenizer_LR.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook can be found here [Logistic Regression, TweetTokenizer and TfidfVectorizer](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/kaggle_competition_logistic_regression_tweet_tokenizer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment prediction with BERT ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After working with the above models, we decided to implement our sentiment classifier using BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Implementation using text cleaning and stemming:**\n",
    "\n",
    "We first used the same class from our first implementation **using text mining techniques** for text cleaning and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "   \n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After text cleaning we created our training and testing dataframes saved into .csv files in a format BERT can understand, it icludes using one-hot encoding labels for each training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/stemming_train.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/stemming_test.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using stemming and text cleaning on tweets we got a score of 47.17% on the kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. Implementation using TweetTokenizer in text:**\n",
    "\n",
    "When using text cleaning and stemming the tweet may lose some important information which will not be helpful for models such BERT to understand the context in sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the tweet as complete as possible we use TweetTokenizer, minor changes occur to the original tweet when the tokenizer is applied, for example the mentions are removed and the punctuation are separated as a single token.\n",
    "\n",
    "The purpose to use the tokenizer was to make the sentence more readable for BERT, so after the sentence was splitted into tokens and some replacements were made we then again convert the tokens list to a single string (tweet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets before applying the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/untokenized_tweet.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets after applying the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/tokenized_tweet.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see minor changes are made to the tweet after it is tokenized and converted back to a single string, this will allow BERT to have a better understanding of the context. (The more noticeable differences on the tweets that can be seen in these two snapshots are the removed mentions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for reference the notebook can be found here [Preprocessing and TweetTokenizer](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/simple_text_preprocessing_TweetTokenizer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this simple cleaning on tweets and training BERT with the following parameters we got a score of **53.788%** and become the top 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncased_L-12_H-768_A-12 ## Pretrained model was used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook for BERT classifier can be found here: [BERT sentiment classifier](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/pred_sentiment_bert.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "**Documented work and commented code:** I icluded all the most relevant work done in the process of doing the homework. All notebooks contain important information and comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
