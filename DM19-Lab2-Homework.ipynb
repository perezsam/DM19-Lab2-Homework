{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Samuel Perez\n",
    "\n",
    "Student ID: 107065434\n",
    "\n",
    "GitHub ID: perezsam\n",
    "\n",
    "Kaggle name: transcendence\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![Snapshot](img/pic0.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/pic1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM19-Lab2-Master Repo](https://github.com/EvaArevalo/DM19-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/179d01d4dd984fc5ac45a894822479dd) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 23rd 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/EvaArevalo/DM19-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM19-Lab2-Homework](https://github.com/EvaArevalo/DM19-Lab2-Homework) repository this time! Also please __DON´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different notebooks were created to keep the different tasks organized**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Completed **take home** exercises can be found in the following notebook [Lab 2 - Take home exercises](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/lab2_take_home_exercises.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "**Kaggle Competition:** Refer to the the leaderboard position showed in the student information section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "I tried different models when working on the competition, to avoid having a big notebook, a link to each notebook will be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using simple tex mining: ###\n",
    "The following notebook [Using text mining techniques](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/kaggle_competition_simple_text_mining.ipynb) include everything from raw data preprocessing, data exploration, text cleaning, grid search for finding the best tuning parameters for the two selected models (i.e MultinomailNB and LogisticRegression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the GridSearchCV from sklearn we found that CountVectorizer was the best choice compared with TF-IDF when working with the LogisticRegression and MultinomailNB models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the best tunning parameters we got 45.3% and 46.11% in the leaderboard for MultinomailNB and LogisticRegression respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-layered fully connected Neural Network ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also tried building a Neural Network, however I was relying only on google colab for building and training the models since I do not have access to a computer with good computational resources.\n",
    "\n",
    "I got the problem of running out of memory, so I couldn't train my model with the full training dataset, besides that the training can take a long time and I can easily loss my work when get disconnected from the colab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following figure we can see that the public score (second column) stay at 40% and there is no significant improvement when increasing the training set sample by a few thousands, except for the improvement from 38% to 40% using a training sample of 50,000 and 90,000 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/nn_model.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a training sample of 250,000 I couldn’t go further due to memory and time constraints from google colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook of the Neural Network model can be found here, [NN twitter sentiment prediction](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/twitter_sentiment_prediction_NN.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LogisticRegression, TweetTokenizer and TfidfVectorizer ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some research I found out that nltk has a TwitterTokenizer wich is a specialized tokenizer for tweets analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize**\n",
    "After tokenizing, we have to use a vectorizer to get angument term matrices (usually sparse). Then angumnt matrices can be sent to a machince learning or deep learning model as inputs. There are two steps in vectorizing processes: fitting and transforming . In fitting part, vectorizers will extract every features from text we provided. In trasformer part, transformer will count frequncies of every features showing up in text, and give a spasrse matrix.\n",
    "\n",
    "**CountVectorizer**\n",
    "BOW model only consider the frequncies of every fetures in texts. The higher the frequencies, the higher the weights. CountVectorizer can be used to implement both BOW (bag of words) and TFIDF (term frequencies inverse document frequencies, with a TFIDF transformer) models. After fitting, we can apply a CountTransformer to get BOW matrices of traning and testing sets.\n",
    "\n",
    "**TfidfVectorizer = CountVectorizer + TfidfTransformer**\n",
    "Since BOW model only consider the frequncies of every fetures in texts, it may lose some context and meaning infromation. TFIDF model not only consider term frequencies in current texts, it also take term frequencies in other texts. If a word is common in every text, for example, \"Friday\", it may be not so important for distinguish emotions. TFIDF will reduce the weights of those common features to make other special important term higher weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some experiments with the lab dataset to check the performance our LogisticRegression model using TwitterTokenizer and TfidfVectorizer we selected the most appropriate parameters to train our emotion predictor for the kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a score of 46% in the Kaggle competition when trying different maximum features for the TfidfVectorizer. There is no significat change when increasing the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Snapshot](img/twitter_tokenizer_LR.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook can be found here [Logistic Regression, TweetTokenizer and TfidfVectorizer](https://github.com/perezsam/DM19-Lab2-Homework/blob/master/kaggle_competition_logistic_regression_tweet_tokenizer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "**Documented work and commented code:** I tried to include all the relevant work done in the process of doing the homework. All notebooks include some important information and comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
